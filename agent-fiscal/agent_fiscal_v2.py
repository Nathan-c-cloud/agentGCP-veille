"""
Agent Fiscal V2 - Adapt√© pour utiliser la nouvelle collection de chunks
Version optimis√©e pour le RAG avec recherche sur chunks s√©mantiques.
"""

import functions_framework
from flask import jsonify
from google.cloud import firestore
import vertexai
from vertexai.generative_models import GenerativeModel
import os
from typing import List, Dict


# --- Configuration ---
PROJECT_ID = os.environ.get("PROJECT_ID", "agent-gcp-f6005")
LOCATION = "us-west1"

# --- Initialisation ---
vertexai.init(project=PROJECT_ID, location=LOCATION)
db = firestore.Client()
model = GenerativeModel("gemini-2.0-flash")

# --- Param√®tres de recherche ---
MAX_CHUNKS = 10  # Nombre maximum de chunks √† r√©cup√©rer (r√©duit car les chunks sont plus petits et cibl√©s)

# --- Prompt syst√®me ---
PROMPT_SYSTEME = """Tu es un assistant fiscal expert sp√©cialis√© dans la fiscalit√© des PME fran√ßaises.

R√àGLES STRICTES :
1. Tu dois baser tes r√©ponses EXCLUSIVEMENT sur les documents de contexte fournis ci-dessous.
2. Ne r√©ponds JAMAIS √† une question si la r√©ponse n'est pas dans le contexte.
3. Si l'information n'est pas disponible, dis clairement : "Je n'ai pas trouv√© cette information dans ma base de connaissances."
4. Cite toujours la source (titre et URL) des informations que tu utilises.
5. Sois pr√©cis, professionnel et structur√© dans tes r√©ponses.
6. Si plusieurs sources donnent des informations compl√©mentaires, synth√©tise-les de mani√®re coh√©rente.

CONTEXTE DOCUMENTAIRE :
{contexte}

QUESTION DE L'UTILISATEUR :
{question}

R√âPONSE :"""


def extraire_mots_cles(question: str) -> List[str]:
    """
    Extrait les mots-cl√©s pertinents d'une question.
    Version am√©lior√©e avec normalisation et filtrage.
    
    Args:
        question: La question de l'utilisateur
        
    Returns:
        Liste de mots-cl√©s normalis√©s
    """
    # Mots vides fran√ßais √† ignorer
    mots_vides = {
        'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'au', 'aux',
        'et', 'ou', 'mais', 'donc', 'or', 'ni', 'car',
        'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles',
        'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses',
        'ce', 'cet', 'cette', 'ces',
        'qui', 'que', 'quoi', 'dont', 'o√π',
        'est', 'sont', '√™tre', 'avoir', 'faire',
        'pour', 'dans', 'sur', 'avec', 'sans', 'sous', 'par',
        'quoi', 'quel', 'quelle', 'quels', 'quelles',
        'comment', 'combien', 'pourquoi', 'quand',
        'c', 'qu', 'd', 'l', 's', 't', 'n', 'm'
    }
    
    # Normaliser et d√©couper
    question_lower = question.lower()
    mots = question_lower.split()
    
    # Filtrer et nettoyer
    mots_cles = []
    for mot in mots:
        # Retirer la ponctuation
        mot_clean = ''.join(c for c in mot if c.isalnum() or c in ['√©', '√®', '√™', '√†', '√¢', '√π', '√ª', '√¥', '√Æ', '√ß'])
        
        # Garder seulement si pas un mot vide et assez long
        if mot_clean and mot_clean not in mots_vides and len(mot_clean) >= 3:
            mots_cles.append(mot_clean)
    
    return mots_cles


def rechercher_chunks(question: str, max_chunks: int = MAX_CHUNKS) -> List[Dict]:
    """
    Recherche les chunks les plus pertinents pour une question.
    Version optimis√©e pour la nouvelle structure de donn√©es.
    
    Args:
        question: La question de l'utilisateur
        max_chunks: Nombre maximum de chunks √† retourner
        
    Returns:
        Liste de chunks pertinents avec leurs m√©tadonn√©es
    """
    print(f"\nüîç Recherche de chunks pour : '{question}'")
    
    # Extraire les mots-cl√©s
    mots_cles = extraire_mots_cles(question)
    print(f"   Mots-cl√©s extraits : {mots_cles}")
    
    if not mots_cles:
        print("   ‚ö†Ô∏è  Aucun mot-cl√© pertinent trouv√©")
        return []
    
    # Recherche dans Firestore
    collection_ref = db.collection("documents_fiscaux_chunks")
    
    # Strat√©gie de recherche : on cherche les chunks qui contiennent les mots-cl√©s
    # Note: Pour une vraie production, utiliser Vector Search ou Algolia
    chunks_trouves = []
    chunks_scores = {}  # Pour scorer les chunks
    
    # R√©cup√©rer tous les chunks (pour une vraie prod, utiliser une recherche vectorielle)
    # Ici on fait simple pour la d√©mo
    all_chunks = collection_ref.limit(500).stream()  # Limiter pour √©viter les timeouts
    
    for chunk_doc in all_chunks:
        chunk_data = chunk_doc.to_dict()
        contenu = chunk_data.get('contenu', '').lower()
        titre = chunk_data.get('titre_source', '').lower()
        
        # Calculer un score bas√© sur la pr√©sence des mots-cl√©s
        score = 0
        for mot_cle in mots_cles:
            # Bonus si le mot-cl√© est dans le titre
            if mot_cle in titre:
                score += 3
            # Points pour chaque occurrence dans le contenu
            score += contenu.count(mot_cle)
        
        if score > 0:
            chunk_data['score'] = score
            chunks_scores[chunk_doc.id] = score
            chunks_trouves.append(chunk_data)
    
    # Trier par score d√©croissant
    chunks_trouves.sort(key=lambda x: x['score'], reverse=True)
    
    # Limiter au nombre demand√©
    chunks_pertinents = chunks_trouves[:max_chunks]
    
    print(f"   ‚úÖ {len(chunks_pertinents)} chunk(s) trouv√©(s)")
    for i, chunk in enumerate(chunks_pertinents[:3], 1):  # Afficher les 3 premiers
        print(f"      {i}. Score {chunk['score']}: {chunk.get('titre_source', 'Sans titre')[:60]}...")
    
    return chunks_pertinents


def construire_contexte(chunks: List[Dict]) -> str:
    """
    Construit le contexte textuel √† partir des chunks trouv√©s.
    
    Args:
        chunks: Liste de chunks pertinents
        
    Returns:
        Texte format√© du contexte
    """
    if not chunks:
        return "Aucun document pertinent trouv√©."
    
    contexte_parts = []
    
    for i, chunk in enumerate(chunks, 1):
        titre = chunk.get('titre_source', 'Sans titre')
        url = chunk.get('source_url', 'URL non disponible')
        contenu = chunk.get('contenu', '')
        
        contexte_parts.append(f"""
--- Document {i} ---
Titre: {titre}
Source: {url}
Contenu:
{contenu}
""")
    
    return "\n".join(contexte_parts)


def generer_reponse(question: str, contexte: str) -> str:
    """
    G√©n√®re une r√©ponse en utilisant le mod√®le LLM avec le contexte fourni.
    
    Args:
        question: La question de l'utilisateur
        contexte: Le contexte documentaire
        
    Returns:
        La r√©ponse g√©n√©r√©e
    """
    print(f"\nü§ñ G√©n√©ration de la r√©ponse avec le mod√®le LLM...")
    
    # Construire le prompt complet
    prompt = PROMPT_SYSTEME.format(
        contexte=contexte,
        question=question
    )
    
    try:
        # Appeler le mod√®le
        response = model.generate_content(prompt)
        reponse_text = response.text
        
        print(f"   ‚úÖ R√©ponse g√©n√©r√©e ({len(reponse_text)} caract√®res)")
        return reponse_text
        
    except Exception as e:
        print(f"   ‚ùå Erreur lors de la g√©n√©ration : {e}")
        raise


@functions_framework.http
def agent_fiscal(request):
    """
    Point d'entr√©e de la Cloud Function.
    Re√ßoit une question et retourne une r√©ponse bas√©e sur les chunks de la base de connaissances.
    """
    # G√©rer CORS
    if request.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST',
            'Access-Control-Allow-Headers': 'Content-Type',
        }
        return ('', 204, headers)
    
    headers = {
        'Access-Control-Allow-Origin': '*'
    }
    
    try:
        # R√©cup√©rer la question
        request_json = request.get_json(silent=True)
        
        if not request_json or 'question' not in request_json:
            return jsonify({
                "erreur": "Aucune question fournie. Utilisez le format: {\"question\": \"votre question\"}"
            }), 400, headers
        
        question = request_json['question']
        print(f"\n{'='*80}")
        print(f"üì® Question re√ßue : {question}")
        print(f"{'='*80}")
        
        # √âTAPE 1: Rechercher les chunks pertinents
        chunks = rechercher_chunks(question)
        
        if not chunks:
            return jsonify({
                "question": question,
                "reponse": "Je n'ai pas trouv√© d'information pertinente dans ma base de connaissances pour r√©pondre √† cette question.",
                "chunks_trouves": 0
            }), 200, headers
        
        # √âTAPE 2: Construire le contexte
        contexte = construire_contexte(chunks)
        
        # √âTAPE 3: G√©n√©rer la r√©ponse
        reponse = generer_reponse(question, contexte)
        
        # Retourner la r√©ponse
        return jsonify({
            "question": question,
            "reponse": reponse,
            "chunks_trouves": len(chunks),
            "sources": [
                {
                    "titre": chunk.get('titre_source'),
                    "url": chunk.get('source_url')
                }
                for chunk in chunks[:3]  # Retourner les 3 sources principales
            ]
        }), 200, headers
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        return jsonify({
            "erreur": "Erreur lors de la g√©n√©ration de la r√©ponse.",
            "details": str(e)
        }), 500, headers


if __name__ == "__main__":
    # Test local
    print("Test local de l'agent fiscal V2...")
    
    question_test = "C'est quoi la TVA ?"
    
    print(f"\nQuestion de test : {question_test}")
    
    # Simuler la recherche
    chunks = rechercher_chunks(question_test)
    
    if chunks:
        contexte = construire_contexte(chunks)
        print(f"\nContexte construit ({len(contexte)} caract√®res)")
        print("\nPremiers 500 caract√®res du contexte:")
        print("-"*80)
        print(contexte[:500])
        print("-"*80)
    else:
        print("\n‚ö†Ô∏è  Aucun chunk trouv√©")

